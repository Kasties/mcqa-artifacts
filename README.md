# Multiple Choice Question Answering Artifacts (IN PROGRESS)

This repository is the official implementation of the in-progress paper: "Large Language Models Exploit Artifacts in Multiple Choice Question Answering"

<p align="center">
  <img src="/images/figure.png"></img>
</p>

## Overview

This repository contains the code and dataset to run the direct answer and process of elimination strategies, with and without chain of thought, on our four tested commonsense reasoning and scientific reasoning multiple-choice QA datasets.

## Setup

Python 3.10.0, pip 23.2.1, and conda 23.5.0 were used when running the code in this repository. A list of requirements can be found in `requirements.txt`, which can be installed through the following command:
```
pip install -r requirements.txt 
```

The most important files in this repository are as follows:
* `/model/`: Contains the code for running all experiments with LLMs
* `/prompts/`: Contains the prompts used in the independent full and indepdent choices-only prompt experiments
* `/evaluation/`: Contains the code for generating our plots
* `/scripts/`: Sample bash scripts to run our code

## Model Usage

There are six relevant files:
* `/model/run_hf.py`: Code to run all possible prompt formats, except the second step of the inferring the question strategy and the random question prompt
* `/model/run_hf_remote.py`: Version of `run_hf.py` that works with models on the huggingface hub where you need to trust remote code (i.e. Phi)
* `/model/extract_generated_questions.py`: After running step one of the inferring the question strategy in the above two Python scripts, this will extract the questions generated by the LLM. This must be run before either of the last two Python scripts
* `/model/extract_random_questions.py`: This will sample a random set of questions from the test set. for the LLM to use. This must be run before either of the last two Python files
* `/model/run_hf_question.py`: Code to run the second step of the inferring the question strategy and the random question prompt
* `/model/run_hf_question_remote.py`: Version of `run_hf_question.py` that works with models on the huggingface hub where you need to trust remote code (i.e. Phi)


## Evaluation Usage


